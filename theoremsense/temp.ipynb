{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the data\n",
    "data_dir = Path('~/model_outputs').expanduser()\n",
    "# grab each subdirectory\n",
    "model_dirs = [x for x in data_dir.iterdir() if x.is_dir()]\n",
    "model_names = [x.name for x in model_dirs]\n",
    "dataset_names = list(set(x.name for model_dir in model_dirs for x in model_dir.iterdir()))\n",
    "model_names, dataset_names\n",
    "import json\n",
    "\n",
    "# load the name maps\n",
    "with open('name_maps.json', 'r') as f:\n",
    "    name_maps = json.load(f)\n",
    "    DATASET_MAP = name_maps['DATASET_MAP']\n",
    "    MODEL_MAP = name_maps['MODEL_MAP']\n",
    "%%capture\n",
    "from dataset import from_name\n",
    "from prompt import generate_nshot_prompts\n",
    "from dataset import load_datasets, BOXED_ANSWERS_DATASETS\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "datasets_raw = load_datasets(BOXED_ANSWERS_DATASETS)\n",
    "datasets = {\n",
    "    DATASET_MAP[dataset['name']]: dataset['data']['train']\n",
    "    for dataset in datasets_raw\n",
    "}\n",
    "\n",
    "np.random.seed(0)\n",
    "num_samples = 100\n",
    "# datasets_subset = {k: np.random.choice(v, num_samples, replace=False) for k, v in datasets.items()}\n",
    "datasets_subset = datasets\n",
    "\n",
    "datasets = {k: list(v) for k, v in datasets.items()}\n",
    "\n",
    "# prepend datasets_subset with 3 examples from datasets\n",
    "# datasets_subset = {k: datasets[k][:3] + list(v) for k, v in datasets_subset.items()}\n",
    "\n",
    "datas = {k: generate_nshot_prompts(v, n=3) for k, v in datasets_subset.items()}\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# model_path = download_model(\"Unbabel/XCOMET-XL\")\n",
    "model_path = download_model(\"Unbabel/wmt23-cometkiwi-da-xl\")\n",
    "model = load_from_checkpoint(model_path)\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from roscoe.score import (\n",
    "    SEQ_EMB_MODEL_TYPES,\n",
    "    Chain,\n",
    "    Evaluator,\n",
    "    REASONING_SCORES,\n",
    "    UNSUPERVISED_SCORES,\n",
    "    SENT_TRANS,\n",
    "    SIMSCE\n",
    ")\n",
    "from roscoe.util import (\n",
    "    print_and_reset_max_gpu_memory,\n",
    "    save_scores,\n",
    "    split_gsm8k_gpt3_generations_to_steps,\n",
    ")\n",
    "\n",
    "\n",
    "class ReasoningSteps(Chain):\n",
    "    def __init__(self, line: str, type=\"regular\") -> None:\n",
    "        self.chain = self.parse_chain(line, type=type)\n",
    "\n",
    "    def parse_chain(self, chain: str, type: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Change formatting.\n",
    "\n",
    "        Returns list of steps in reasoning chain.\n",
    "        \"\"\"\n",
    "        if type == \"gsm8k_ref\":\n",
    "            return chain.split(\"IGNORE THIS. Ground truth here for reference. \")[\n",
    "                1\n",
    "            ].split('\\n')\n",
    "        elif type == \"gsm8k_hypo\":\n",
    "            return split_gsm8k_gpt3_generations_to_steps(reasoning=chain)\n",
    "        elif type == \"regular\":\n",
    "            return sent_tokenize(chain)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{type} chain type is not supported\")\n",
    "\n",
    "\n",
    "# get memory usage of evaluator.ppl_model\n",
    "param_size = 0\n",
    "for param in evaluator.grmr_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in evaluator.grmr_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024 ** 3\n",
    "print('model size: {:.3f}GB'.format(size_all_mb))\n",
    "from roscoe.score import Evaluator\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    score_types=REASONING_SCORES,\n",
    "    model_type=SIMSCE,\n",
    "    transformer_model=\"facebook/roscoe-512-roberta-base\",\n",
    "    ppl_model=\"gpt2-large\",\n",
    "    discourse_batch=64,\n",
    "    coherence_batch=16,\n",
    "    hypos=[],\n",
    "    context=[],\n",
    ")\n",
    "len(evaluator.hypos), len(evaluator.context), len(evaluator.references)\n",
    "\n",
    "\n",
    "def get_nshot_base_question(question):\n",
    "    return question.split('4.')[-1].strip()\n",
    "\n",
    "\n",
    "def compute_roscoe(outputs, solutions):\n",
    "    # hypos is the predicted reasoning chain\n",
    "    hypos = [\n",
    "        ReasoningSteps(o.outputs[0].text)\n",
    "        for o in outputs\n",
    "    ]\n",
    "    # refs is the ground truth reasoning chain\n",
    "    refs = [\n",
    "        ReasoningSteps(s['answer'] if isinstance(s, dict) else s)\n",
    "        for s in solutions\n",
    "    ]\n",
    "    # context is the prompt\n",
    "    context = [\n",
    "        ReasoningSteps(get_nshot_base_question(o.prompt))\n",
    "        for o in outputs\n",
    "    ]\n",
    "\n",
    "    # x = [get_nshot_base_question(s['question']) for s in solutions]\n",
    "    # y = [get_nshot_base_question(o.prompt) for o in outputs]\n",
    "    # print(len(hypos), len(refs), len(context))\n",
    "    # print(x[:2])\n",
    "    # print('=' * 20)\n",
    "    # print(y[:2])\n",
    "    # raise ValueError\n",
    "    # evaluate\n",
    "    n = 50\n",
    "    evaluator.set_hypos(hypos[:n])\n",
    "    evaluator.set_references(refs[:n])\n",
    "    evaluator.set_context(context[:n])\n",
    "    scores = evaluator.evaluate()\n",
    "    return dict(scores)\n",
    "\n",
    "\n",
    "# roscoe_res = {\n",
    "#     dataset: compute_roscoe(pickle.load(open(subdir / \"deepseek-ai_deepseek-math-7b-instruct_autoregressive.pkl\", 'rb'))[0], datas[dataset.replace(' ', '_')])\n",
    "#     for dataset, subdir in zip(dataset_names, subdirs)\n",
    "# }\n",
    "with open('/lfs/skampere1/0/kaif/generated_outputs/roscoe_temp.pkl', 'wb') as f:\n",
    "    pickle.dump(roscoe_res, f)\n",
    "\n",
    "\n",
    "# data = [\n",
    "#     # {\n",
    "#     #     \"src\": datas['Hendrycks Algebra'][0]['question'],\n",
    "#     #     \"mt\": predictions[0].outputs[0].text,\n",
    "#     #     \"ref\": datas['Hendrycks Algebra'][0]['answer']\n",
    "#     # }\n",
    "#     {\n",
    "#         \"src\": d['question'],\n",
    "#         \"mt\": o.outputs[0].text,\n",
    "#         \"ref\": d['answer']\n",
    "#     } for d, o in zip(datas['Hendrycks Algebra'], predictions)\n",
    "# ]\n",
    "# model_output = model.predict(data, batch_size=64, gpus=1)\n",
    "# # Segment-level scores\n",
    "# print (model_output.scores)\n",
    "#\n",
    "# # System-level score\n",
    "# print (model_output.system_score)\n",
    "#\n",
    "# # Score explanation (error spans)\n",
    "# print (model_output.metadata.error_spans)\n",
    "\n",
    "def comet(outputs, solutions, use_ref=True):\n",
    "    if use_ref:\n",
    "        data = [\n",
    "            {\n",
    "                \"src\": o.prompt,\n",
    "                \"mt\": o.outputs[0].text,\n",
    "                \"ref\": sol\n",
    "            } for (sol, o) in zip(solutions, outputs)\n",
    "        ]\n",
    "    else:\n",
    "        data = [\n",
    "            {\n",
    "                \"src\": o.prompt,\n",
    "                \"mt\": o.outputs[0].text\n",
    "            } for o in outputs\n",
    "        ]\n",
    "    model_output = model.predict(data, batch_size=16, gpus=1)\n",
    "    return model_output\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# xcomet_res = {\n",
    "#     dataset: xcomet(pickle.load(open(subdir / \"deepseek-ai_deepseek-math-7b-instruct_autoregressive.pkl\", 'rb'))[0], datas[dataset])\n",
    "#     for dataset, subdir in zip(dataset_names, subdirs)\n",
    "# }\n",
    "# # save\n",
    "# with open('/lfs/skampere1/0/kaif/generated_outputs/xcom_temp.pkl', 'wb') as f:\n",
    "#     pickle.dump(xcomet_res, f)\n",
    "# load\n",
    "with open('/lfs/skampere1/0/kaif/generated_outputs/xcom_temp.pkl', 'rb') as f:\n",
    "    xcomet_res = pickle.load(f)\n",
    "xcomet_res['Hendrycks Number Theory'][0]\n",
    "import pickle\n",
    "\n",
    "x = pickle.load(open(\n",
    "    \"/lfs/skampere1/0/kaif/generated_outputs/EleutherAI_hendrycks_math_algebra/deepseek-ai_deepseek-math-7b-instruct_autoregressive.pkl\",\n",
    "    'rb'))\n",
    "subdirs[\n",
    "    0] / \"deepseek-ai_deepseek-math-7b-instruct_autoregressive.pkl\", \"/lfs/skampere1/0/kaif/generated_outputs/EleutherAI_hendrycks_math_algebra/deepseek-ai_deepseek-math-7b-instruct_autoregressive.pkl\"\n",
    "from grader import ExactMatchGrader, NextTokenAccuracyGrader\n",
    "from dataset import get_boxed_answer\n",
    "from latex_formater import latex_deformat\n",
    "\n",
    "\n",
    "def grade_predictions(outputs, data):\n",
    "    grader = ExactMatchGrader()\n",
    "\n",
    "    boxed_predictions = []\n",
    "    for o in outputs:\n",
    "        try:\n",
    "            boxed_predictions.append(latex_deformat(get_boxed_answer(o.outputs[0].text)))\n",
    "        except:\n",
    "            print(get_boxed_answer(o.outputs[0].text))\n",
    "\n",
    "    # boxed_predictions = [latex_deformat(get_boxed_answer(o.outputs[0].text)) for o in outputs]\n",
    "    boxed_answers = [latex_deformat(get_boxed_answer(d)) for d in data]\n",
    "    grades = grader.grade(boxed_predictions, boxed_answers)\n",
    "\n",
    "    return grades\n",
    "\n",
    "\n",
    "def teacher_forcing_accuracy(predictions, solutions):\n",
    "    grader = NextTokenAccuracyGrader()\n",
    "    return grader.grade(predictions, solutions)\n",
    "\n",
    "\n",
    "model_names\n",
    "\n",
    "\n",
    "# examine some of the predictions\n",
    "def get_predictions(model, dataset, data_type):\n",
    "    model_dir = data_dir / model\n",
    "    dataset_dir = model_dir / dataset\n",
    "    file = dataset_dir / f\"{data_type}.pkl\"\n",
    "    with open(file, 'rb') as f:\n",
    "        predictions, solutions = pickle.load(f)\n",
    "    return predictions, solutions\n",
    "\n",
    "\n",
    "predictions, solutions = get_predictions('LLeMMA-7b', 'Algebra', 'autoregressive')\n",
    "Llama - 2 - 13\n",
    "b\n",
    "tends\n",
    "to\n",
    "have\n",
    "a\n",
    "bunch\n",
    "of\n",
    "whitespace and ORs in the\n",
    "predictions.\n",
    "Tora - 13\n",
    "b - v1\n",
    ".0\n",
    "lots\n",
    "of\n",
    "code and repeated\n",
    "sections.Probably\n",
    "hard\n",
    "to\n",
    "parse\n",
    "into\n",
    "steps\n",
    "i = 0\n",
    "print(get_nshot_base_question(predictions[i].prompt))\n",
    "print('=' * 20)\n",
    "print(solutions[i])\n",
    "print('=' * 20)\n",
    "print(predictions[i].outputs[0].text)\n",
    "root_dir = Path('/lfs/skampere1/0/kaif/model_outputs')\n",
    "for model_dir in root_dir.iterdir():\n",
    "    any_ouputs = False\n",
    "    for dataset_dir in model_dir.iterdir():\n",
    "        num_files = len(list(dataset_dir.iterdir()))\n",
    "        if num_files != 0:\n",
    "            any_ouputs = True\n",
    "    if not any_ouputs:\n",
    "        print(f\"Empty directory: {model_dir}\")\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "HOOKS = {\n",
    "    'autoregressive': {\n",
    "        'Accuracy': grade_predictions,\n",
    "        'ROSCOE': compute_roscoe,\n",
    "        # 'XCOMET': comet\n",
    "    }\n",
    "    # 'teacher_forcing': {}\n",
    "}\n",
    "\n",
    "\n",
    "def ensure_dict_path(d, path):\n",
    "    \"\"\"\n",
    "    Ensure that the path exists in the dictionary\n",
    "    \"\"\"\n",
    "    for key in path:\n",
    "        if key not in d:\n",
    "            d[key] = {}\n",
    "        d = d[key]\n",
    "\n",
    "\n",
    "def eval_data(data_dir):\n",
    "    evals = {}\n",
    "    number_of_models = len(list(data_dir.iterdir()))\n",
    "    number_of_metrics = sum(len(metrics) for metrics in HOOKS.values())\n",
    "    with (\n",
    "        tqdm(total=number_of_models, desc='Models') as model_tqdm,\n",
    "        tqdm(total=0, desc='Dataset') as data_tqdm,\n",
    "        tqdm(total=number_of_metrics, desc='Metric') as metric_tqdm\n",
    "    ):\n",
    "        for model_dir in data_dir.iterdir():\n",
    "            model_name = model_dir.name\n",
    "            model_tqdm.set_description(model_name)\n",
    "            data_tqdm.reset(total=len(list(model_dir.iterdir())))\n",
    "\n",
    "            for dataset_dir in model_dir.iterdir():\n",
    "                dataset_name = dataset_dir.name\n",
    "                data_tqdm.set_description(dataset_name)\n",
    "                metric_tqdm.reset()\n",
    "\n",
    "                for data_type, metrics in HOOKS.items():\n",
    "                    for metric_name, metric in metrics.items():\n",
    "                        metric_tqdm.set_description(metric_name)\n",
    "                        file = dataset_dir / f\"{data_type}.pkl\"\n",
    "                        if file.exists():\n",
    "                            with open(file, 'rb') as f:\n",
    "                                predictions, solutions = pickle.load(f)\n",
    "                                metric_value = metric(predictions, solutions)\n",
    "\n",
    "                                if isinstance(metric_value, dict):\n",
    "                                    for k, v in metric_value.items():\n",
    "                                        sub_metric_name = f\"{metric_name}_{k}\"\n",
    "                                        ensure_dict_path(evals, [sub_metric_name, dataset_name, model_name])\n",
    "                                        evals[sub_metric_name][dataset_name][model_name] = v\n",
    "                                else:\n",
    "                                    ensure_dict_path(evals, [metric_name, dataset_name, model_name])\n",
    "                                    evals[metric_name][dataset_name][model_name] = metric_value\n",
    "                        metric_tqdm.update(1)\n",
    "                data_tqdm.update(1)\n",
    "            model_tqdm.update(1)\n",
    "\n",
    "    return evals\n",
    "\n",
    "\n",
    "evals = eval_data(root_dir)\n",
    "evals.keys()\n",
    "import pickle\n",
    "\n",
    "save_path = Path('/lfs/skampere1/0/kaif/generated_outputs/metrics/evals_roscoe.pkl')\n",
    "# save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# with open(save_path, 'wb') as f:\n",
    "#     pickle.dump(evals, f)\n",
    "with open(save_path, 'rb') as f:\n",
    "    evals = pickle.load(f)\n",
    "for metric, datasets in evals.items():\n",
    "    for dataset, models in datasets.items():\n",
    "        for model, value in models.items():\n",
    "            try:\n",
    "                x = np.mean(value)\n",
    "            except:\n",
    "                print(len([v for v in value if isinstance(v, float)]),\n",
    "                      np.mean([v for v in value if isinstance(v, float)]), metric, dataset, model)\n",
    "                if len([v for v in value if isinstance(v, float)]) == 0:\n",
    "                    print(value)\n",
    "                    print('=' * 20)\n",
    "roscoe_avg\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def sum_values(a, b):\n",
    "    a = [v if isinstance(v, float | bool) else 0 for v in a]\n",
    "    b = [v if isinstance(v, float | bool) else 0 for v in b]\n",
    "    return [x + y for x, y in zip(a, b)]\n",
    "\n",
    "\n",
    "# compute average of all metrics with ROSCOE in their name\n",
    "roscoe_avg = {}\n",
    "n = 0\n",
    "for metric, datasets in evals.items():\n",
    "    if 'ROSCOE' in metric:\n",
    "        n += 1\n",
    "        roscoe_avg = {\n",
    "            dataset: {\n",
    "                model: sum_values(v, roscoe_avg.get(dataset, {}).get(model, [0] * len(v)))\n",
    "                for model, v in models.items()\n",
    "            }\n",
    "            for dataset, models in datasets.items()\n",
    "        }\n",
    "roscoe_avg = {\n",
    "    dataset: {\n",
    "        model: [v / n for v in value]\n",
    "        for model, value in models.items()\n",
    "    }\n",
    "    for dataset, models in roscoe_avg.items()\n",
    "}\n",
    "evals['ROSCOE'] = roscoe_avg\n",
    "\n",
    "evals_avg = {\n",
    "    metric: {\n",
    "        model: {\n",
    "            dataset: np.mean([v for v in value if isinstance(v, float | bool)])\n",
    "            for dataset, value in datasets.items()\n",
    "        }\n",
    "        for model, datasets in models.items()\n",
    "    }\n",
    "    for metric, models in evals.items()\n",
    "}\n",
    "\n",
    "evals_dfs = {\n",
    "    metric: pd.DataFrame(v).melt(value_name=metric, var_name='dataset',\n",
    "                                 ignore_index=False).reset_index(names='model')\n",
    "    for metric, v in evals_avg.items()\n",
    "}\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# plot histograms for each metric, model, dataset conditioned on accuracy\n",
    "def plot_histograms(evals, metric, model, dataset, ax):\n",
    "    # check to see if dataset and model are in evals\n",
    "    ax.set_title(f\"{model} {dataset}\")\n",
    "    if dataset not in evals[metric] or model not in evals[metric][dataset]:\n",
    "        return\n",
    "    metric_data = evals[metric][dataset][model]\n",
    "    metric_data = [m if isinstance(m, float | bool) else -0.5 for m in metric_data]\n",
    "    accuracy_data = evals['Accuracy'][dataset][model]\n",
    "    metric_correct = [m for m, a in zip(metric_data, accuracy_data) if a]\n",
    "    metric_incorrect = [m for m, a in zip(metric_data, accuracy_data) if not a]\n",
    "    ax.hist(metric_correct, bins=20, alpha=0.5, label='Correct')\n",
    "    ax.hist(metric_incorrect, bins=20, alpha=0.5, label='Incorrect')\n",
    "    # set x ticks to be the same\n",
    "    ax.set_xticks(np.linspace(-0.5, 1, 5))\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "# plot_histograms(evals, 'ROSCOE', 'Deepseek-7b-Instruct', 'Algebra', plt.gca())\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "for metric in evals_dfs.keys():\n",
    "    fig, axs = plt.subplots(len(model_names), len(dataset_names),\n",
    "                            figsize=(len(model_names) * 4, len(dataset_names) * 8))\n",
    "    for i, model in enumerate(model_names):\n",
    "        for j, dataset in enumerate(dataset_names):\n",
    "            plot_histograms(evals, metric, model, dataset, axs[i, j])\n",
    "    plt.savefig(f'plots/hist_{metric}.png')\n",
    "    plt.close()\n",
    "# merge all the dataframes in evals_dfs\n",
    "from functools import reduce\n",
    "\n",
    "df = reduce(lambda x, y: pd.merge(x, y, on=['dataset', 'model']), evals_dfs.values())\n",
    "for metric in evals_dfs.values():\n",
    "    assert df.shape[0] == metric.shape[0]\n",
    "\n",
    "# compute R^2 of each metric against Accuracy conditioned on model (use .corr().iloc[0, 1] ** 2)\n",
    "R2_model = {\n",
    "    metric: df.groupby('model').apply(lambda x: x[['Accuracy', metric]].corr().iloc[0, 1] ** 2)\n",
    "    for metric in evals_dfs.keys()\n",
    "}\n",
    "R2_dataset = {\n",
    "    metric: df.groupby('dataset').apply(lambda x: x[['Accuracy', metric]].corr().iloc[0, 1] ** 2)\n",
    "    for metric in evals_dfs.keys()\n",
    "}\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# g = sns.lmplot(\n",
    "#     data=df,\n",
    "#     x='Teacher Forced Cross Entropy',\n",
    "#     y='Accuracy',\n",
    "#     hue='model',\n",
    "#     col='dataset',\n",
    "#     col_wrap=3,\n",
    "#     height=4,\n",
    "#     aspect=1,\n",
    "#     scatter_kws=dict(s=50, linewidths=1, edgecolor='w'),\n",
    "#     palette='tab10',\n",
    "# )\n",
    "# g.set(xscale=\"log\", yscale=\"log\")\n",
    "# # put R2 next to model name in legend\n",
    "# # df['Log Accuracy'] = np.log(df['Accuracy'])\n",
    "# xcom_corr = df.groupby('model').apply(lambda x: x[['COMETKIWI', 'Accuracy']].corr().iloc[0, 1] ** 2)\n",
    "#\n",
    "# g = sns.lmplot(\n",
    "#     data=df,\n",
    "#     x='COMETKIWI',\n",
    "#     y='Accuracy',\n",
    "#     hue='model',\n",
    "#     height=6,\n",
    "#     aspect=1.5,\n",
    "#     scatter_kws=dict(s=50, linewidths=1, edgecolor='w'),\n",
    "#     palette='tab10',\n",
    "#     # ci=None,\n",
    "#     # legend=False,  # turn off the original legend\n",
    "# )\n",
    "#\n",
    "# import matplotlib.patches as mpatches\n",
    "#\n",
    "# # Fetching the color palette based on the number of unique models\n",
    "# color_palette = sns.color_palette('tab10', n_colors=len(df['model'].unique()))\n",
    "#\n",
    "# legend_patches = []\n",
    "# for i, model in enumerate([l.get_text() for l in g.legend.texts]):\n",
    "#     correlation = xcom_corr[model]\n",
    "#     legend_patches.append(mpatches.Patch(color=color_palette[i],\n",
    "#                                          label=f'({correlation:0.2f}) {model}'))\n",
    "#\n",
    "# g.legend.set_visible(False)\n",
    "# plt.legend(handles=legend_patches)\n",
    "# plt.show()\n",
    "# # g.set(xscale=\"log\", yscale=\"log\")\n",
    "\n",
    "# repeat above but for each df in evals_dfs\n",
    "for metric, eval_df in evals_dfs.items():\n",
    "    g = sns.lmplot(\n",
    "        data=df,\n",
    "        x=metric,\n",
    "        y='Accuracy',\n",
    "        hue='model',\n",
    "        height=6,\n",
    "        aspect=1.5,\n",
    "        scatter_kws=dict(s=50, linewidths=1, edgecolor='w'),\n",
    "        palette='tab10',\n",
    "        ci=None,\n",
    "        # legend=False,  # turn off the original legend\n",
    "    )\n",
    "\n",
    "    import matplotlib.patches as mpatches\n",
    "\n",
    "    # Fetching the color palette based on the number of unique models\n",
    "    color_palette = sns.color_palette('tab10', n_colors=len(df['model'].unique()))\n",
    "\n",
    "    legend_patches = []\n",
    "    for i, model in enumerate([l.get_text() for l in g.legend.texts]):\n",
    "        correlation = R2_model[metric][model]\n",
    "        legend_patches.append(mpatches.Patch(color=color_palette[i],\n",
    "                                             label=f'({correlation:0.2f}) {model}'))\n",
    "\n",
    "    g.legend.set_visible(False)\n",
    "    plt.legend(handles=legend_patches)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
