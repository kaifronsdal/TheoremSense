{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:45:23.595984Z",
     "start_time": "2024-02-26T18:45:21.073402Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "# load transformers model openai-community/gpt2\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "# from transformers import AwqConfig\n",
    "# quantization_config = AwqConfig(\n",
    "#     bits=4,\n",
    "#     fuse_max_seq_len=512,\n",
    "#     do_fuse=True,\n",
    "# )\n",
    "import torch\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55da96baac69827",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-26T18:45:50.354375Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# name = 'deepseek-ai/deepseek-math-7b-instruct'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(name).to(device)\n",
    "# model = AutoModelForCausalLM.from_pretrained(name, attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16).to(0)\n",
    "name = 'lmsys/vicuna-7b-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "# pipe = pipeline('text-generation', model=name, tokenizer=name, device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "#                 batch_size=10, attn_implementation=\"flash_attention_2\")\n",
    "# pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "# generate_config = dict(do_sample=True, temperature=0.8, top_p=0.95,\n",
    "#                        max_new_tokens=512, num_return_sequences=1, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "# tokenizer = pipe.tokenizer\n",
    "# model = pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "120915a68ed2ee30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:46:52.264520Z",
     "start_time": "2024-02-26T18:46:52.255610Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c5f6dc26d8c74c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:50:37.777414Z",
     "start_time": "2024-02-26T18:50:37.769615Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '', '1', '2', '3', '4', '5']\n",
      "['<s>', '', '1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "prompts = ['12345']\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "print([tokenizer.decode([i]) for i in inputs['input_ids'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f898dc319d00366a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:50:20.364295Z",
     "start_time": "2024-02-26T18:50:20.349131Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question = \"What is the meaning of life?\"\n",
    "prompts = ['12345', '123']\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a15655408122daa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T18:50:07.913314Z",
     "start_time": "2024-02-26T18:50:07.902022Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '', '1', '2', '3', '4', '5']\n",
      "['<s>', '', '1', '2', '3', '</s>', '</s>']\n",
      "['<s>', '', '1', '2', '3', '4', '5']\n",
      "['<s>', '', '1', '2', '3', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inputs.input_ids)):\n",
    "    print([tokenizer.decode([i]) for i in inputs['input_ids'][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a68755ea1d78ad05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T00:07:39.725465Z",
     "start_time": "2024-02-23T00:07:38.100899Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life?\n",
      "The meaning and purpose of our lives is a question that has been asked by many people throughout history. The answer to this question can vary depending on the individual and their perspective. Some people believe that the purpose\n"
     ]
    }
   ],
   "source": [
    "# Generate regular predictions\n",
    "response = model.generate(inputs.to(device), max_length=50, no_repeat_ngram_size=2, do_sample=False)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d693dd88892ad349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T22:38:16.000610Z",
     "start_time": "2024-02-15T22:38:15.254841Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life? No one knows the meaning of life. It is a mystery. Some people believe that the purpose of our lives is to seek happiness, while others believe it is our duty to serve others or to fulfill a specific\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the meaning of life? No one knows the meaning of life. It is a mystery.\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\").input_ids\n",
    "response = model.generate(inputs.to(device), max_length=50, no_repeat_ngram_size=2, do_sample=False)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51b1d3302d016ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T00:08:13.232373Z",
     "start_time": "2024-02-23T00:08:12.179978Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from dataset import from_name\n",
    "from prompt import generate_nshot_prompts\n",
    "\n",
    "data = from_name('EleutherAI/hendrycks_math', subset='algebra')['data']\n",
    "data = generate_nshot_prompts(data['train'], 3)\n",
    "prompts = [d['question'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ae2c047bb1ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T00:00:13.296167Z",
     "start_time": "2024-02-23T00:00:13.282219Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaForCausalLM"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286c5960c3265aa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T00:08:15.403288Z",
     "start_time": "2024-02-23T00:08:14.315949Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['attn_implementation'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/TheoremSense/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/TheoremSense/lib/python3.10/site-packages/transformers/pipelines/base.py:1177\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1174\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1175\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/conda/envs/TheoremSense/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/TheoremSense/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/TheoremSense/lib/python3.10/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/conda/envs/TheoremSense/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/conda/envs/TheoremSense/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/TheoremSense/lib/python3.10/site-packages/transformers/generation/utils.py:1350\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(generation_config)\n\u001b[1;32m   1349\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[0;32m-> 1350\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/conda/envs/TheoremSense/lib/python3.10/site-packages/transformers/generation/utils.py:1167\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1170\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['attn_implementation'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompts[:10], **generate_config)\n",
    "               # max_new_tokens=512, num_return_sequences=1,\n",
    "               # do_sample=True, temperature=0.8, top_p=0.95)\n",
    "#, pad_token_id=pipe.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1b2d798cb143fa99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T21:56:18.386525Z",
     "start_time": "2024-02-15T21:56:18.372051Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use teacher forcing to generate predictions for a prompt, solution pair\n",
    "def generate_prediction(prompt, solution, model, tokenizer, debug=False):\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    solution_ids = tokenizer(solution, return_tensors=\"pt\").input_ids\n",
    "    response = model(torch.cat([prompt_ids, solution_ids], dim=-1))\n",
    "    # get the response ids for the solution part of the input\n",
    "    tf_ids = torch.argmax(response.logits, dim=-1)[0, prompt_ids.shape[1] - 1:]\n",
    "    if debug:\n",
    "        for i in range(len(tf_ids) - 1):\n",
    "            print(f\"Predicted:\\t{tokenizer.decode([tf_ids[i]], skip_special_tokens=True).__repr__()}\")\n",
    "            print(f\"Actual:   \\t{tokenizer.decode(solution_ids[0, i], skip_special_tokens=True).__repr__()}\")\n",
    "\n",
    "    return tf_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a706f6096fdcfc76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T21:56:19.910234Z",
     "start_time": "2024-02-15T21:56:19.837141Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\t' 8'\n",
      "Actual:   \t' 8'\n",
      "Predicted:\t' 9'\n",
      "Actual:   \t' 9'\n",
      "Predicted:\t' 10'\n",
      "Actual:   \t' 10'\n",
      "Predicted:\t' 11'\n",
      "Actual:   \t' 11'\n",
      "Predicted:\t' 12'\n",
      "Actual:   \t' 12'\n",
      "Predicted:\t' 13'\n",
      "Actual:   \t' 13'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"1 2 3 4 5 6 7\"\n",
    "solution = \" 8 9 10 11 12 13\"\n",
    "tf_ids = generate_prediction(prompt, solution, model, tokenizer, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b0c88ed7cc476b3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T22:16:26.264942Z",
     "start_time": "2024-02-15T22:16:26.213291Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "??model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "725dbaa011916054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:17:53.739549Z",
     "start_time": "2024-02-16T01:17:53.727143Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a batched version of generate_prediction\n",
    "def generate_predictions(prompts, solutions, model, tokenizer, debug=False, device=\"cuda\"):\n",
    "    input = tokenizer([f\"{p}{s}\" for p, s in zip(prompts, solutions)], return_tensors=\"pt\", padding=True,\n",
    "                      truncation=True)\n",
    "    prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # solution_tokens = tokenizer(solutions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    response = model(input.input_ids.to(device), attention_mask=input.attention_mask.to(device))\n",
    "    # get the response ids for the solution part of the input\n",
    "    all_preds = torch.argmax(response.logits, dim=-1)\n",
    "\n",
    "    tf_ids = []\n",
    "    solution_ids = []\n",
    "    for i in range(len(prompts)):\n",
    "        # assumes attention mask is contiguous\n",
    "        prompt_length = prompt_tokens.attention_mask[i].sum().item()\n",
    "        solution_start = torch.nonzero(input.attention_mask[i, :] == 1, as_tuple=False)[0, 0].item() + prompt_length\n",
    "        solution_length = input.attention_mask[i].sum().item() - prompt_length\n",
    "\n",
    "        assert prompt_tokens.input_ids[i, -1] == input.input_ids[i, solution_start - 1]\n",
    "\n",
    "        # print(input.attention_mask[i])\n",
    "        # print(f'prompt_tokens[i]: {[tokenizer.decode([t]) for t in prompt_tokens.input_ids[i]]}')\n",
    "        # print(f'input[i]: {[tokenizer.decode([t]) for t in input.input_ids[i]]}')\n",
    "        # print(f'all_preds[i]: {[tokenizer.decode([t]) for t in all_preds[i]]}')\n",
    "        # print(f'solution[i]: {[tokenizer.decode([t]) for t in input.input_ids[i, solution_start:solution_start + solution_length]]}')\n",
    "        # print(f'predicted: {[tokenizer.decode([t]) for t in all_preds[i, solution_start - 1:solution_start - 1 + solution_length]]}')\n",
    "\n",
    "        tf_ids.append(all_preds[i, solution_start - 1:solution_start - 1 + solution_length])\n",
    "        solution_ids.append(input.input_ids[i, solution_start:solution_start + solution_length])\n",
    "\n",
    "    if debug:\n",
    "        for i in range(len(tf_ids)):\n",
    "            print(f\"Prompt: {prompts[i]}\")\n",
    "            assert len(tf_ids[i]) == len(solution_ids[i])\n",
    "            for j in range(len(tf_ids[i])):\n",
    "                print(f\"Predicted:\\t{tokenizer.decode([tf_ids[i][j]], skip_special_tokens=True).__repr__()}\")\n",
    "                print(f\"Actual:   \\t{tokenizer.decode(solution_ids[i][j], skip_special_tokens=True).__repr__()}\")\n",
    "\n",
    "    return tf_ids, solution_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8c6b264309fb1d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:17:55.238110Z",
     "start_time": "2024-02-16T01:17:55.168700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 12345\n",
      "Predicted:\t'\\n'\n",
      "Actual:   \t'6'\n",
      "Predicted:\t'7'\n",
      "Actual:   \t'7'\n",
      "Predicted:\t'8'\n",
      "Actual:   \t'8'\n",
      "Predicted:\t'9'\n",
      "Actual:   \t'9'\n",
      "Predicted:\t'1'\n",
      "Actual:   \t'1'\n",
      "Predicted:\t'0'\n",
      "Actual:   \t'0'\n",
      "Prompt: 123\n",
      "Predicted:\t'4'\n",
      "Actual:   \t'4'\n",
      "Predicted:\t'5'\n",
      "Actual:   \t'5'\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"12345\", \"123\"]\n",
    "solutions = [\"678910\", \"45\"]\n",
    "tf_ids = generate_predictions(prompts, solutions, model, tokenizer, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "id": "df1171bd0fd00554",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
