{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:04:33.517890Z",
     "start_time": "2024-02-22T22:04:30.171895Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "# load transformers model openai-community/gpt2\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "gen_config = GenerationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d55da96baac69827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:13:46.321252Z",
     "start_time": "2024-02-22T22:13:42.655122Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7103573f26d24112af35ad0b5b32b1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "name = 'deepseek-ai/deepseek-math-7b-instruct'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(name).to(device)\n",
    "pipe = pipeline('text-generation', model=name, tokenizer=name, device_map=\"auto\", torch_dtype=torch.bfloat16,\n",
    "                batch_size=10)\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "generate_config = dict(do_sample=True, temperature=0.8, top_p=0.95,\n",
    "                       max_new_tokens=512, num_return_sequences=1, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "tokenizer = pipe.tokenizer\n",
    "model = pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "120915a68ed2ee30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T22:40:54.669464Z",
     "start_time": "2024-02-15T22:40:54.664421Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f898dc319d00366a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:13:50.657414Z",
     "start_time": "2024-02-22T22:13:50.650392Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100000,   2640,    317,    254,   4569,    280,   1728,     30]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the meaning of life?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\").input_ids\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68755ea1d78ad05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:13:56.503860Z",
     "start_time": "2024-02-22T22:13:55.392333Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life?\n",
      "The meaning and purpose of your life is something that you have to find for yourself. It is not something you can be told or handed to you on a silver platter. The meaning is a journey that is\n"
     ]
    }
   ],
   "source": [
    "# Generate regular predictions\n",
    "response = model.generate(inputs.to(device), max_length=50, no_repeat_ngram_size=2, do_sample=False)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d693dd88892ad349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T22:38:16.000610Z",
     "start_time": "2024-02-15T22:38:15.254841Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life? No one knows the meaning of life. It is a mystery. Some people believe that the purpose of our lives is to seek happiness, while others believe it is our duty to serve others or to fulfill a specific\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the meaning of life? No one knows the meaning of life. It is a mystery.\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\").input_ids\n",
    "response = model.generate(inputs.to(device), max_length=50, no_repeat_ngram_size=2, do_sample=False)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51b1d3302d016ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:04:41.915109Z",
     "start_time": "2024-02-22T22:04:40.525365Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hendrycks_math (/lfs/skampere1/0/kaif/.cache/huggingface/datasets/EleutherAI___hendrycks_math/algebra/0.0.1/170db4d4d7d6e523b75159227fc93bd5368d3025cf38d8be579c0b3d6199c952)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76807e968e1347058f020b29156165a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /lfs/skampere1/0/kaif/.cache/huggingface/datasets/EleutherAI___hendrycks_math/algebra/0.0.1/170db4d4d7d6e523b75159227fc93bd5368d3025cf38d8be579c0b3d6199c952/cache-3141018808f2bcbe.arrow\n",
      "Loading cached processed dataset at /lfs/skampere1/0/kaif/.cache/huggingface/datasets/EleutherAI___hendrycks_math/algebra/0.0.1/170db4d4d7d6e523b75159227fc93bd5368d3025cf38d8be579c0b3d6199c952/cache-db6a524eab4515da.arrow\n"
     ]
    }
   ],
   "source": [
    "from dataset import from_name\n",
    "from prompt import generate_nshot_prompts\n",
    "\n",
    "data = from_name('EleutherAI/hendrycks_math', subset='algebra')['data']\n",
    "data = generate_nshot_prompts(data['train'], 3)\n",
    "prompts = [d['question'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286c5960c3265aa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T22:04:58.635798Z",
     "start_time": "2024-02-22T22:04:41.777984Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs = pipe(prompts[:10], **generate_config)\n",
    "               # max_new_tokens=512, num_return_sequences=1,\n",
    "               # do_sample=True, temperature=0.8, top_p=0.95)\n",
    "#, pad_token_id=pipe.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1b2d798cb143fa99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T21:56:18.386525Z",
     "start_time": "2024-02-15T21:56:18.372051Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use teacher forcing to generate predictions for a prompt, solution pair\n",
    "def generate_prediction(prompt, solution, model, tokenizer, debug=False):\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    solution_ids = tokenizer(solution, return_tensors=\"pt\").input_ids\n",
    "    response = model(torch.cat([prompt_ids, solution_ids], dim=-1))\n",
    "    # get the response ids for the solution part of the input\n",
    "    tf_ids = torch.argmax(response.logits, dim=-1)[0, prompt_ids.shape[1] - 1:]\n",
    "    if debug:\n",
    "        for i in range(len(tf_ids) - 1):\n",
    "            print(f\"Predicted:\\t{tokenizer.decode([tf_ids[i]], skip_special_tokens=True).__repr__()}\")\n",
    "            print(f\"Actual:   \\t{tokenizer.decode(solution_ids[0, i], skip_special_tokens=True).__repr__()}\")\n",
    "\n",
    "    return tf_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a706f6096fdcfc76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T21:56:19.910234Z",
     "start_time": "2024-02-15T21:56:19.837141Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\t' 8'\n",
      "Actual:   \t' 8'\n",
      "Predicted:\t' 9'\n",
      "Actual:   \t' 9'\n",
      "Predicted:\t' 10'\n",
      "Actual:   \t' 10'\n",
      "Predicted:\t' 11'\n",
      "Actual:   \t' 11'\n",
      "Predicted:\t' 12'\n",
      "Actual:   \t' 12'\n",
      "Predicted:\t' 13'\n",
      "Actual:   \t' 13'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"1 2 3 4 5 6 7\"\n",
    "solution = \" 8 9 10 11 12 13\"\n",
    "tf_ids = generate_prediction(prompt, solution, model, tokenizer, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b0c88ed7cc476b3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T22:16:26.264942Z",
     "start_time": "2024-02-15T22:16:26.213291Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "??model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "725dbaa011916054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:17:53.739549Z",
     "start_time": "2024-02-16T01:17:53.727143Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a batched version of generate_prediction\n",
    "def generate_predictions(prompts, solutions, model, tokenizer, debug=False, device=\"cuda\"):\n",
    "    input = tokenizer([f\"{p}{s}\" for p, s in zip(prompts, solutions)], return_tensors=\"pt\", padding=True,\n",
    "                      truncation=True)\n",
    "    prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # solution_tokens = tokenizer(solutions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    response = model(input.input_ids.to(device), attention_mask=input.attention_mask.to(device))\n",
    "    # get the response ids for the solution part of the input\n",
    "    all_preds = torch.argmax(response.logits, dim=-1)\n",
    "\n",
    "    tf_ids = []\n",
    "    solution_ids = []\n",
    "    for i in range(len(prompts)):\n",
    "        # assumes attention mask is contiguous\n",
    "        prompt_length = prompt_tokens.attention_mask[i].sum().item()\n",
    "        solution_start = torch.nonzero(input.attention_mask[i, :] == 1, as_tuple=False)[0, 0].item() + prompt_length\n",
    "        solution_length = input.attention_mask[i].sum().item() - prompt_length\n",
    "\n",
    "        assert prompt_tokens.input_ids[i, -1] == input.input_ids[i, solution_start - 1]\n",
    "\n",
    "        # print(input.attention_mask[i])\n",
    "        # print(f'prompt_tokens[i]: {[tokenizer.decode([t]) for t in prompt_tokens.input_ids[i]]}')\n",
    "        # print(f'input[i]: {[tokenizer.decode([t]) for t in input.input_ids[i]]}')\n",
    "        # print(f'all_preds[i]: {[tokenizer.decode([t]) for t in all_preds[i]]}')\n",
    "        # print(f'solution[i]: {[tokenizer.decode([t]) for t in input.input_ids[i, solution_start:solution_start + solution_length]]}')\n",
    "        # print(f'predicted: {[tokenizer.decode([t]) for t in all_preds[i, solution_start - 1:solution_start - 1 + solution_length]]}')\n",
    "\n",
    "        tf_ids.append(all_preds[i, solution_start - 1:solution_start - 1 + solution_length])\n",
    "        solution_ids.append(input.input_ids[i, solution_start:solution_start + solution_length])\n",
    "\n",
    "    if debug:\n",
    "        for i in range(len(tf_ids)):\n",
    "            print(f\"Prompt: {prompts[i]}\")\n",
    "            assert len(tf_ids[i]) == len(solution_ids[i])\n",
    "            for j in range(len(tf_ids[i])):\n",
    "                print(f\"Predicted:\\t{tokenizer.decode([tf_ids[i][j]], skip_special_tokens=True).__repr__()}\")\n",
    "                print(f\"Actual:   \\t{tokenizer.decode(solution_ids[i][j], skip_special_tokens=True).__repr__()}\")\n",
    "\n",
    "    return tf_ids, solution_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8c6b264309fb1d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:17:55.238110Z",
     "start_time": "2024-02-16T01:17:55.168700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 12345\n",
      "Predicted:\t'\\n'\n",
      "Actual:   \t'6'\n",
      "Predicted:\t'7'\n",
      "Actual:   \t'7'\n",
      "Predicted:\t'8'\n",
      "Actual:   \t'8'\n",
      "Predicted:\t'9'\n",
      "Actual:   \t'9'\n",
      "Predicted:\t'1'\n",
      "Actual:   \t'1'\n",
      "Predicted:\t'0'\n",
      "Actual:   \t'0'\n",
      "Prompt: 123\n",
      "Predicted:\t'4'\n",
      "Actual:   \t'4'\n",
      "Predicted:\t'5'\n",
      "Actual:   \t'5'\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"12345\", \"123\"]\n",
    "solutions = [\"678910\", \"45\"]\n",
    "tf_ids = generate_predictions(prompts, solutions, model, tokenizer, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "id": "df1171bd0fd00554",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
